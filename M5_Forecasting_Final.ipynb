{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c931675",
   "metadata": {},
   "source": [
    "# M5 Forecasting: Final Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd094f3",
   "metadata": {},
   "source": [
    "# **Contents**\n",
    "\n",
    "<h3> 1. Define helper functions </h3> \n",
    "\n",
    "<h3> 2. Create Final Pipelines </h3>\n",
    "\n",
    "<h3> 3. Check the output </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e24d37c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random \n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e2985",
   "metadata": {},
   "source": [
    "# 1. Define helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da99552",
   "metadata": {},
   "source": [
    "## 1.1 Downcast Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd809fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#refer - https://stackoverflow.com/questions/1658714/how-to-get-the-range-of-valid-numpy-data-types\n",
    "def downcast(data):\n",
    "    cols = data.columns\n",
    "    for col in cols:\n",
    "        if data[col].dtype == object:\n",
    "            if col =='date':\n",
    "                data[col] = pd.to_datetime(data[col])\n",
    "            else: \n",
    "                data[col] = data[col].astype('category') \n",
    " \n",
    "        #only check the upper value because we only have positive values in dataframes\n",
    "        elif data[col].dtype == int: \n",
    "            if data[col].max() < np.iinfo('int8').max:\n",
    "                data[col] = data[col].astype('int8')\n",
    "\n",
    "            elif data[col].max() < np.iinfo('int16').max:\n",
    "                data[col] = data[col].astype('int16')\n",
    "\n",
    "            elif data[col].max() < np.iinfo('int32').max:\n",
    "                data[col] = data[col].astype('int32')\n",
    "            else:\n",
    "                data[col] = data[col].astype('int64')\n",
    "\n",
    "        elif data[col].dtype == float:\n",
    "            if data[col].max() < np.finfo('float16').max:\n",
    "                data[col] = data[col].astype('float16')\n",
    "            elif data[col].max() < np.finfo('float32').max:\n",
    "                data[col] = data[col].astype('float32')\n",
    "            else:\n",
    "                data[col] = data[col].astype('float64')\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a0ed3b",
   "metadata": {},
   "source": [
    "## 1.2 Compute Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95407755",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_groupings = {2: [\"state_id\"], 3: [\"store_id\"], 4: [\"cat_id\"], 5: [\"dept_id\"], 6: [\"state_id\", \"cat_id\"], 7: [\"state_id\", \"dept_id\"],\n",
    "                   8: [\"store_id\", \"cat_id\"], 9: [\"store_id\", \"dept_id\"], 10: [\"item_id\"], 11: [\"item_id\", \"state_id\"]}\n",
    "\n",
    "def calculate_weightsL12(sales, cal, price, last28):\n",
    "    ''' Calculate weights for level 12 (Product-Store) series' using the last 28 days sales data '''\n",
    "\n",
    "    #calculating weights for level 12 : 'item_id, store_id' using last 28 days of train data\n",
    "    #this loop is repeated 28 times to get the sales revenue of all ids for each of last 28 days \n",
    "    #flow of execution : day-> week id-> sell price of ids on the day-> sales revenue of ids for the day\n",
    "    for day in range(last28[0], last28[1]):\n",
    "        #get the week id corresponding to the day \n",
    "        week_id = int(cal[cal[\"d_\"]==day][\"wm_yr_wk\"]) \n",
    "\n",
    "        #get the week price for each of the items corresponding to the week id   \n",
    "        week_price = price[price[\"wm_yr_wk\"]==week_id]\n",
    "\n",
    "        #merge sales with week price on 'id'\n",
    "        #note: we merge the dataframes using inner join so the id which are present in both dataframes will be retained after merging\n",
    "        sales = sales.merge(week_price[[\"sell_price\", \"id\"]], on=[\"id\"], how='inner')\n",
    "\n",
    "        #create a column which shows the sales revenue for the day\n",
    "        #sales revenue = sell_price * units_sold \n",
    "        sales[\"sales_revenue_d_\" + str(day)] = sales[\"sell_price\"] * sales[\"d_\" + str(day)]\n",
    "\n",
    "        #drop the sell_price column\n",
    "        sales.drop(columns=[\"sell_price\"], inplace=True)\n",
    "\n",
    "    #Sum of sales revenue of each id for last 28 days   1`\n",
    "    sales_revenue_cols = [x for x in sales.columns if x.find(\"sales_revenue\")==0]\n",
    "    sales['sales_revenue_alldays'] = sales[sales_revenue_cols].sum(axis=1)\n",
    "\n",
    "    #Compute weights for each Level 12 Time Series\n",
    "    sales['weight'] = (1/12)*(sales['sales_revenue_alldays']/sales['sales_revenue_alldays'].sum())\n",
    "\n",
    "    #Drop the unnecessary columns \n",
    "    sales.drop(columns = sales_revenue_cols+['sales_revenue_alldays'], inplace=True)\n",
    "\n",
    "    return sales\n",
    "\n",
    "def calculate_weightsALL(sales, levels):\n",
    "    ''' Calculate weights for series' in rest of the aggregation levels '''\n",
    "    \n",
    "    #weights for level 1 : 'all'\n",
    "    agg = pd.DataFrame(sales[[x for x in sales.columns if x.find(\"d_\") == 0 or x.find(\"F\") == 0]].sum()).transpose() \n",
    "    id_cols = [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n",
    "    for col in id_cols:\n",
    "        agg[col] = 'all'\n",
    "    agg[\"level\"] = 1\n",
    "    agg[\"weight\"] = 1/12\n",
    "    column_order = agg.columns\n",
    "\n",
    "    #weights for the rest of the levels (levels 2-11)\n",
    "    for level in level_groupings:\n",
    "        temp_df = sales.groupby(by=level_groupings[level]).sum().reset_index()\n",
    "        temp_df[\"level\"] = level\n",
    "        \n",
    "        for c in column_order:\n",
    "            if c not in temp_df.columns:\n",
    "                temp_df[c] = 'all'\n",
    "                \n",
    "        agg = agg.append(temp_df[column_order])\n",
    "\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b150e8",
   "metadata": {},
   "source": [
    "## 1.3 Compute WRMSSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c0097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_day_splits(sales, train_start, train_end, val_start, val_end):\n",
    "    '''Create lists of days/columns to be selected for train, val and forecast when calculating RMSSE'''\n",
    "    \n",
    "    train_days =  [x for x in sales.columns if x.find(\"d_\") == 0 and int(x.split(\"_\")[1]) in range(train_start, train_end+1)] \n",
    "    val_days =  [x for x in sales.columns if x.find(\"d_\") == 0 and int(x.split(\"_\")[1]) in range(val_start, val_end+1)]\n",
    "    forecast_days = [x for x in sales.columns if x.find(\"F\") == 0]\n",
    "    return train_days, val_days, forecast_days\n",
    "\n",
    "def RMSSE(ground_truth, forecast, train, n, h):\n",
    "    ''' Calculates the RMSSE score for all series in the dataframe. '''\n",
    "\n",
    "    num = ((ground_truth - forecast)**2).sum(axis=1)\n",
    "    den = 1/(n-1) * ((train[:, 1:] - train[:, :-1]) ** 2).sum(axis=1)  \n",
    "    rmsse = (1/h * num/den) ** 0.5\n",
    "\n",
    "    return rmsse\n",
    "\n",
    "def WRMSSE(sales, agg, train_days, val_days, forecast_days, n, h):\n",
    "    ''' Calculates the WRMSSE score for the model prediction '''\n",
    "    \n",
    "    ground_truth_df = np.array(sales[val_days])\n",
    "    forecast_df = np.array(sales[forecast_days])\n",
    "    train_df = np.array(sales[train_days])\n",
    "\n",
    "    ground_truth_agg_df = np.array(agg[val_days])\n",
    "    forecast_agg_df = np.array(agg[forecast_days])\n",
    "    train_agg_df = np.array(agg[train_days])\n",
    "            \n",
    "    # calculate rmsse for each series using df[val-days] - df[forecast-days] ->numerator   \n",
    "    sales[\"rmsse\"] = RMSSE(ground_truth_df, forecast_df, train_df, n, h)\n",
    "    agg[\"rmsse\"] = RMSSE(ground_truth_agg_df, forecast_agg_df, train_agg_df, n, h)\n",
    "\n",
    "    sales[\"wrmsse\"] = sales[\"weight\"] * sales[\"rmsse\"]\n",
    "    agg[\"wrmsse\"] = agg[\"weight\"] * agg[\"rmsse\"]\n",
    "\n",
    "    wrmsse = sales[\"wrmsse\"].sum() + agg[\"wrmsse\"].sum()\n",
    "    \n",
    "    return wrmsse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad30628",
   "metadata": {},
   "source": [
    "## 1.4 Feature Engineering\n",
    "- We create 2 separate feature engineering functions for Validation and Evaluation days because the inputs and order of steps differ a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48af17e",
   "metadata": {},
   "source": [
    "## 1.4.1 Feature Engineering (Evaluation Days: 1942 to 1969)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94cdf144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featEng_eval(X):\n",
    "    ''' Takes list of level 12 ids as input and returns long format data with all features for days in evaluation phase '''\n",
    "    \n",
    "    train_end = 1941\n",
    "    \n",
    "    df_s = pd.read_csv('sales_train_evaluation.csv')\n",
    "    df_p = pd.read_csv('sell_prices.csv')\n",
    "    df_c = pd.read_csv('calendar.csv', parse_dates=['date'])\n",
    "    \n",
    "    '''\n",
    "    Basic Data Manipulations: \n",
    "    1. Convert the day column names from string to number (Sales Dataframe)\n",
    "    2. Add days/columns for evaluation period (Sales Dataframe)\n",
    "    3. Convert day column dtype from object to integer and replace null values in event columns (Calender Dataframe)\n",
    "    4. Create id feature combining store and item id (Price Dataframe) \n",
    "    '''\n",
    "    df = df_s.loc[df_s['id'].isin(X)]\n",
    "    \n",
    "    #1.\n",
    "    df.columns = list(df.columns[:6]) + list(range(1,train_end+1))\n",
    "\n",
    "    #2.\n",
    "    for day in range(train_end+1, train_end+28+1):\n",
    "        df[day] = 0\n",
    "\n",
    "    #3.\n",
    "    df_c[\"d\"]= df_c[\"d\"].apply(lambda x: int(x.split(\"_\")[1])).astype('int16')\n",
    "\n",
    "    cal_catFeat = ['event_name_1','event_type_1','event_name_2','event_type_2']\n",
    "\n",
    "    for feat in cal_catFeat:\n",
    "        df_c[feat] = df_c[feat].replace(np.nan, -1)\n",
    "    \n",
    "    #4.\n",
    "    df_p[\"id\"] = df_p[\"item_id\"] + \"_\" + df_p[\"store_id\"] + \"_evaluation\"\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Basic Feature Creation (Time Based features):\n",
    "    1. Derive quarter, week and day features from date.\n",
    "    2. Create a binary is_weekend feature whose function is self explanatory    \n",
    "    '''\n",
    "    #1.\n",
    "    datetimeFeat = [\"quarter\", \"week\", \"day\"] \n",
    "    for feat in datetimeFeat:\n",
    "        df_c[feat] = getattr(df_c['date'].dt, feat).astype('int8')\n",
    "\n",
    "    df_c.rename(columns = {'day':'day_of_month'}, inplace=True)\n",
    "\n",
    "    #2.\n",
    "    df_c['is_weekend'] = df_c[\"weekday\"].apply(lambda x: 1 if x in ['Saturday','Sunday'] else 0).astype('int8')\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Transformations and Feature Creation:\n",
    "    1. Convert the data to long format\n",
    "    2. Create lag features derived from sales\n",
    "    3. Create rolling window based statistical features on sales (mean)\n",
    "    4. Merge dataframes\n",
    "    5. Create price lag and change features \n",
    "    '''\n",
    "    #1.\n",
    "    df = pd.melt(df, id_vars = [c for c in df.columns if type(c)==str], var_name='d', value_name='sales')\n",
    "    df['d'] = pd.to_numeric(df['d']).astype('int16')\n",
    "    \n",
    "    #drop majority of historical data to reduce computation time of features\n",
    "    #because we only need preprocessed data for validation days we can drop most of previous data\n",
    "    df = (df[df['d']>=1700]).reset_index(drop=True)\n",
    "    \n",
    "    #2.\n",
    "    lags = [28, 30, 31, 35, 42, 49, 56, 63, 70, 77]\n",
    "    \n",
    "    for lag in tqdm(lags):\n",
    "        df[f'lag_{lag}'] = df.groupby([\"id\"])[\"sales\"].shift(lag).astype('float16')\n",
    "        \n",
    "    #3.\n",
    "    windows = [7, 14, 28, 30, 45, 60, 90, 120]\n",
    "    \n",
    "    for window in tqdm(windows):\n",
    "        df[f\"rmean_28_{window}\"] = df.groupby([\"id\"])[\"lag_28\"].transform(lambda x: x.rolling(window).mean()).astype('float16')\n",
    "    \n",
    "    #4.\n",
    "    df = pd.merge(df, df_c.drop(columns=['weekday']), how='left', on='d')\n",
    "    df = pd.merge(df, df_p.drop(columns=['store_id', 'item_id']), how='left', on=['id', 'wm_yr_wk'])\n",
    "    \n",
    "    #5. \n",
    "    df['price_shift_t1'] = df.groupby(['id'])['sell_price'].shift(1)\n",
    "    df['price_change_t1'] = (df['sell_price'] - df['price_shift_t1'])/df['price_shift_t1']\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Data Manipulation:\n",
    "    1. Retain data after day 1941 and drop the rest\n",
    "    2. Convert dtype of lag features to 'int16' \n",
    "    3. Replace null values in price based features with 0 \n",
    "    '''\n",
    "    #1.\n",
    "    df = (df[(df['d']>=(train_end+1)) & (df['d']<=(train_end+28))]).reset_index(drop=True)\n",
    "    \n",
    "    #2.\n",
    "    for feat in [\"lag_28\", \"lag_30\", \"lag_31\", \"lag_35\", \"lag_42\", \"lag_49\", \"lag_56\", \"lag_63\", \"lag_70\", \"lag_77\"]:\n",
    "        df[feat] = df[feat].astype('int16')\n",
    "    \n",
    "    #3.\n",
    "    for feat in ['sell_price','price_shift_t1','price_change_t1']:\n",
    "        df[feat] = df[feat].replace(np.nan, 0)\n",
    "    \n",
    "    '''\n",
    "    Categorical Feature Encoding:\n",
    "    1. Load the category dicts \n",
    "    2. Use category dicts to encode categorical features\n",
    "    '''\n",
    "    sales_catFeat = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "    cal_catFeat = ['event_name_1','event_type_1','event_name_2','event_type_2']\n",
    "\n",
    "    #1.\n",
    "    for feat in sales_catFeat+cal_catFeat:\n",
    "        varStr = f'dict_{feat}'\n",
    "        var = vars()\n",
    "        var[varStr] = pickle.load(open(f\"saved_dicts/dict_{feat}.pkl\", \"rb\"))\n",
    "    \n",
    "    #2.\n",
    "    for feat in sales_catFeat+cal_catFeat:\n",
    "        varStr = f'dict_{feat}'\n",
    "        df[feat] = df[feat].apply(lambda x: var[varStr][x] if type(x)==str else -1)\n",
    "    \n",
    "    '''\n",
    "    Downcasting Dataframe\n",
    "    '''\n",
    "    df = downcast(df)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f013c7f",
   "metadata": {},
   "source": [
    "## 1.4.2 Feature Engineering (Validation Days: 1914 to 1941)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2bb21fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featEng_val(X):\n",
    "    ''' Takes level 12 series sales data in wide format as an input and returns long format data with added features for days \n",
    "        in validation phase '''\n",
    "\n",
    "    train_end = 1913\n",
    "    \n",
    "    df_sales = X.copy()\n",
    "    df_price = pd.read_csv('sell_prices.csv')\n",
    "    df_cal = pd.read_csv('calendar.csv', parse_dates=['date'])\n",
    "\n",
    "    '''\n",
    "     Data Manipulations and Feature Creation: \n",
    "     1. Convert the day column names from string to number (Sales Dataframe)\n",
    "     2. Add days/columns for evaluation period (Sales Dataframe)\n",
    "     3. Convert day column dtype from object to integer and add time based features (Calender Dataframe)\n",
    "     4. Create id feature combining store and item id (Price Dataframe) \n",
    "     '''\n",
    "    #1.\n",
    "    df_sales.columns = list(df_sales.columns[:6]) + list(range(1,train_end+1))\n",
    "    \n",
    "    #2.\n",
    "    for day in range(train_end+1, train_end+28+1):\n",
    "        df_sales[day] = 0\n",
    "    \n",
    "    #3.\n",
    "    df_cal[\"d\"]= df_cal[\"d\"].apply(lambda x: int(x.split(\"_\")[1])).astype('int16')\n",
    "    \n",
    "    datetimeFeat = [\"quarter\", \"week\", \"day\"] \n",
    "    for feat in datetimeFeat:\n",
    "        df_cal[feat] = getattr(df_cal['date'].dt, feat).astype('int8')\n",
    "\n",
    "    df_cal.rename(columns = {'day':'day_of_month'}, inplace=True)\n",
    "\n",
    "    df_cal['is_weekend'] = df_cal[\"weekday\"].apply(lambda x: 1 if x in ['Saturday','Sunday'] else 0).astype('int8')\n",
    "\n",
    "    #4.\n",
    "    df_price[\"id\"] = df_price[\"item_id\"] + \"_\" + df_price[\"store_id\"] + \"_evaluation\"\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Downcasting Dataframes\n",
    "    '''\n",
    "    df_sales = downcast(df_sales)\n",
    "    df_cal = downcast(df_cal)\n",
    "    df_price = downcast(df_price)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Feature encoding\n",
    "    '''\n",
    "    sales_catFeat = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "    cal_catFeat = ['event_name_1','event_type_1','event_name_2','event_type_2']\n",
    "    \n",
    "    for col in sales_catFeat:\n",
    "        df_sales[col] = df_sales[col].cat.codes\n",
    "        if col == 'id':\n",
    "            df_price[col] = df_price[col].cat.codes\n",
    "\n",
    "    for col in cal_catFeat:\n",
    "        df_cal[col] = df_cal[col].cat.codes\n",
    "\n",
    "        \n",
    "    '''\n",
    "    Downcasting Dataframes\n",
    "    '''\n",
    "    df_sales = downcast(df_sales)\n",
    "    df_cal = downcast(df_cal)\n",
    "    df_price = downcast(df_price)\n",
    "\n",
    "    \n",
    "    '''\n",
    "     Transformations and Feature Creation:\n",
    "     1. Convert the data to long format\n",
    "     2. Create lag features derived from sales\n",
    "     3. Create rolling window based statistical features on sales (mean)\n",
    "     4. Merge dataframes\n",
    "     5. Create price lag and change features \n",
    "     '''\n",
    "    #1.\n",
    "    df_long = pd.melt(df_sales, id_vars = ['id','item_id','dept_id','cat_id','store_id','state_id'], var_name='d', value_name='sales')\n",
    "    df_long['d'] = pd.to_numeric(df_long['d']).astype('int16')\n",
    "    \n",
    "    #drop majority of historical data to reduce computation time of features\n",
    "    #because we only need preprocessed data for evaluation days we can drop most of previous data\n",
    "    df_long = (df_long[df_long['d']>=1700]).reset_index(drop=True)\n",
    "    \n",
    "    #2.\n",
    "    lags = [28, 30, 31, 35, 42, 49, 56, 63, 70, 77]\n",
    "    for lag in tqdm(lags):\n",
    "        df_long[f'lag_{lag}'] = df_long.groupby([\"id\"])['sales'].shift(lag).astype('float16')\n",
    "\n",
    "    #3.\n",
    "    windows = [7, 14, 28, 30, 45, 60, 90, 120]\n",
    "    for window in tqdm(windows):\n",
    "        df_long[f\"rmean_28_{window}\"] = df_long.groupby([\"id\"])[\"lag_28\"].transform(lambda x: x.rolling(window).mean()).astype('float16')\n",
    "\n",
    "    #4.\n",
    "    df_long = pd.merge(df_long, df_cal.drop(columns=['weekday']), how='left', on='d')\n",
    "    df_long = pd.merge(df_long, df_price.drop(columns=['store_id', 'item_id']), how='left', on=['id', 'wm_yr_wk'])\n",
    "\n",
    "    #5.\n",
    "    df_long['price_shift_t1'] = df_long.groupby(['id'])['sell_price'].shift(1)\n",
    "    df_long['price_change_t1'] = (df_long['sell_price'] - df_long['price_shift_t1'])/df_long['price_shift_t1']\n",
    "\n",
    "    \n",
    "    '''\n",
    "    Data Manipulation:\n",
    "    1. Retain data after day 1913 and drop the rest\n",
    "    2. Convert dtype of lag features to 'int16' \n",
    "    3. Replace null values in price based features with 0 \n",
    "    '''\n",
    "    \n",
    "    #1.\n",
    "    df_long = (df_long[df_long['d']>train_end]).reset_index(drop=True)\n",
    "    \n",
    "    #2.\n",
    "    for feat in [\"lag_28\", \"lag_30\", \"lag_31\", \"lag_35\", \"lag_42\", \"lag_49\", \"lag_56\", \"lag_63\", \"lag_70\", \"lag_77\"]:\n",
    "        df_long[feat] = df_long[feat].astype('int16')\n",
    "    #3.    \n",
    "    for feat in ['sell_price','price_shift_t1','price_change_t1']:\n",
    "        df_long[feat] = df_long[feat].replace(np.nan, 0)\n",
    "    \n",
    "    '''\n",
    "    Downcast Final Dataframe\n",
    "    '''\n",
    "    df_long = downcast(df_long)\n",
    "    \n",
    "    return df_long"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1361adf5",
   "metadata": {},
   "source": [
    "# 2. Create Final Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a25de8",
   "metadata": {},
   "source": [
    "## 2.1 Forecast Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4f8ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales forecasting for evaluation days i.e. 1942 to 1969\n",
    "def forecastPipe(X):\n",
    "    ''' Takes list of level 12 ids as input and returns sales forecast for days 1942 to 1969 '''\n",
    "    train_start, train_end = 1069, 1941\n",
    "    \n",
    "    #load the model\n",
    "    model = pickle.load(open(\"model_eval.pkl\", \"rb\"))\n",
    "    \n",
    "    #get the long format dataframe with added features to be used as input to the model\n",
    "    print(\"<<<Preprocessing>>>\")\n",
    "    X_q = featEng_eval(X)\n",
    "    \n",
    "    #create a list of features to be selected\n",
    "    removeFeat = ['d', 'sales', 'date', 'wm_yr_wk']\n",
    "    features = [c for c in X_q.columns if c not in removeFeat]\n",
    "    \n",
    "    df_sales = pd.read_csv('sales_train_evaluation.csv')\n",
    "    \n",
    "    #create a forecast dataframe with id column containing the ids in X\n",
    "    df_q = df_sales.loc[df_sales['id'].isin(X), ['id']]\n",
    "    \n",
    "    #generate forecasts\n",
    "    print(\"<<<Generating Forecasts>>>\")\n",
    "    for day in range(1,29):       \n",
    "        df_q[f\"F{day}\"] = model.predict(X_q.loc[X_q[\"d\"]==(train_end+day), features])\n",
    "        \n",
    "    df_q.reset_index(drop=True, inplace=True)\n",
    "    print(\"<<<Forecast Dataframe Created>>>\")\n",
    "    \n",
    "    return df_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b988fef6",
   "metadata": {},
   "source": [
    "## 2.2 Performance Metric Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "904c0d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales forecasting and computing performance for validation days i.e. 1914 to 1941\n",
    "def performancePipe(X, y):\n",
    "    ''' Takes level 12 series sales data along with target as input and returns WRMSSE score \n",
    "        on forecast made for days 1914 to 1941 '''\n",
    "    \n",
    "    train_start, train_end = 1069, 1913\n",
    "    #load the model\n",
    "    model = pickle.load(open(\"model_val.pkl\", \"rb\"))\n",
    "\n",
    "    #get the long format dataframe with added features to be used as input to the model\n",
    "    print(\"<<<Preprocessing>>>\")\n",
    "    X_val = featEng_val(X)\n",
    "    \n",
    "    #create a list of features to be selected\n",
    "    removeFeat = ['d', 'sales', 'date', 'wm_yr_wk']\n",
    "    features = [c for c in X_val.columns if c not in removeFeat]\n",
    "    \n",
    "    #load calendar and price dataframe and perform basic data manipulation\n",
    "    df_cal = pd.read_csv('calendar.csv', parse_dates=['date'])\n",
    "    df_cal[\"d_\"]=df_cal[\"d\"].apply(lambda x: int(x.split(\"_\")[1]))\n",
    "    \n",
    "    df_price = pd.read_csv('sell_prices.csv')\n",
    "    df_price[\"id\"] = df_price[\"item_id\"] + \"_\" + df_price[\"store_id\"] + \"_evaluation\"\n",
    "    \n",
    "    #concat historical sales data and target data for wrmsse calculation\n",
    "    concat_df = pd.concat([X,y], axis=1)\n",
    "    \n",
    "    #add forecast data to the same concat dataframe\n",
    "    print('<<<Generating Forecasts>>>')\n",
    "    for day in range(1,29):\n",
    "        concat_df[f\"F{day}\"] = model.predict(X_val.loc[X_val[\"d\"]==(train_end+day), features])\n",
    "    \n",
    "    print('<<<Calculating Weights>>>')\n",
    "    concat_df = calculate_weightsL12(concat_df, df_cal, df_price, (train_end - (28 - 1), train_end+1)) \n",
    "    agg_df = calculate_weightsALL(concat_df, level_groupings)\n",
    "\n",
    "    print('<<<Calculating WRMSSE Score>>>')\n",
    "    train_days, val_days, forecast_days = get_day_splits(concat_df, train_start, train_end, val_start = train_end + 1, val_end = train_end + 28)\n",
    "    wrmsse = WRMSSE(concat_df, agg_df, train_days, val_days, forecast_days,  n = ((train_end - train_start)+1) , h = 28)\n",
    "    print('<<<Model Evaluated>>>')\n",
    "    \n",
    "    return wrmsse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2cbaa",
   "metadata": {},
   "source": [
    "# 3. Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "656f9fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = pd.read_csv('sales_train_evaluation.csv')\n",
    "df_price = pd.read_csv('sell_prices.csv')\n",
    "df_cal = pd.read_csv('calendar.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf360ed4",
   "metadata": {},
   "source": [
    "## 3.1 Forecast Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b58d5f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ['HOBBIES_1_004_CA_1_evaluation', 'HOUSEHOLD_1_004_CA_1_evaluation', 'FOODS_3_825_WI_3_evaluation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2c98c0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<Preprocessing>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 1021.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 379.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<Generating Forecasts>>>\n",
      "<<<Forecast Dataframe Created>>>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>1.904641</td>\n",
       "      <td>1.532269</td>\n",
       "      <td>1.585795</td>\n",
       "      <td>1.648829</td>\n",
       "      <td>1.926328</td>\n",
       "      <td>2.320598</td>\n",
       "      <td>2.625637</td>\n",
       "      <td>2.119428</td>\n",
       "      <td>1.699053</td>\n",
       "      <td>...</td>\n",
       "      <td>1.790031</td>\n",
       "      <td>2.283375</td>\n",
       "      <td>2.168417</td>\n",
       "      <td>1.927081</td>\n",
       "      <td>1.420443</td>\n",
       "      <td>1.450095</td>\n",
       "      <td>1.472359</td>\n",
       "      <td>1.613477</td>\n",
       "      <td>2.247674</td>\n",
       "      <td>2.446868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOUSEHOLD_1_004_CA_1_evaluation</td>\n",
       "      <td>1.342333</td>\n",
       "      <td>1.266069</td>\n",
       "      <td>1.035864</td>\n",
       "      <td>1.024562</td>\n",
       "      <td>1.553377</td>\n",
       "      <td>2.073160</td>\n",
       "      <td>1.839534</td>\n",
       "      <td>1.471478</td>\n",
       "      <td>1.334622</td>\n",
       "      <td>...</td>\n",
       "      <td>1.848246</td>\n",
       "      <td>1.741816</td>\n",
       "      <td>1.964079</td>\n",
       "      <td>1.310458</td>\n",
       "      <td>1.431128</td>\n",
       "      <td>1.185328</td>\n",
       "      <td>1.219093</td>\n",
       "      <td>1.659835</td>\n",
       "      <td>2.439339</td>\n",
       "      <td>2.160877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>0.716163</td>\n",
       "      <td>0.626379</td>\n",
       "      <td>0.644473</td>\n",
       "      <td>0.647437</td>\n",
       "      <td>0.776059</td>\n",
       "      <td>0.854815</td>\n",
       "      <td>0.882837</td>\n",
       "      <td>0.754092</td>\n",
       "      <td>0.658399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869791</td>\n",
       "      <td>1.020404</td>\n",
       "      <td>1.035002</td>\n",
       "      <td>0.913090</td>\n",
       "      <td>0.836115</td>\n",
       "      <td>0.908147</td>\n",
       "      <td>0.707877</td>\n",
       "      <td>0.831467</td>\n",
       "      <td>0.881552</td>\n",
       "      <td>0.920081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id        F1        F2        F3        F4  \\\n",
       "0    HOBBIES_1_004_CA_1_evaluation  1.904641  1.532269  1.585795  1.648829   \n",
       "1  HOUSEHOLD_1_004_CA_1_evaluation  1.342333  1.266069  1.035864  1.024562   \n",
       "2      FOODS_3_825_WI_3_evaluation  0.716163  0.626379  0.644473  0.647437   \n",
       "\n",
       "         F5        F6        F7        F8        F9  ...       F19       F20  \\\n",
       "0  1.926328  2.320598  2.625637  2.119428  1.699053  ...  1.790031  2.283375   \n",
       "1  1.553377  2.073160  1.839534  1.471478  1.334622  ...  1.848246  1.741816   \n",
       "2  0.776059  0.854815  0.882837  0.754092  0.658399  ...  0.869791  1.020404   \n",
       "\n",
       "        F21       F22       F23       F24       F25       F26       F27  \\\n",
       "0  2.168417  1.927081  1.420443  1.450095  1.472359  1.613477  2.247674   \n",
       "1  1.964079  1.310458  1.431128  1.185328  1.219093  1.659835  2.439339   \n",
       "2  1.035002  0.913090  0.836115  0.908147  0.707877  0.831467  0.881552   \n",
       "\n",
       "        F28  \n",
       "0  2.446868  \n",
       "1  2.160877  \n",
       "2  0.920081  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecastPipe(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd2fd8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30493</th>\n",
       "      <td>HOBBIES_1_004_CA_1_evaluation</td>\n",
       "      <td>1.904641</td>\n",
       "      <td>1.532269</td>\n",
       "      <td>1.585795</td>\n",
       "      <td>1.648829</td>\n",
       "      <td>1.926328</td>\n",
       "      <td>2.320598</td>\n",
       "      <td>2.625637</td>\n",
       "      <td>2.119428</td>\n",
       "      <td>1.699053</td>\n",
       "      <td>...</td>\n",
       "      <td>1.790031</td>\n",
       "      <td>2.283375</td>\n",
       "      <td>2.168417</td>\n",
       "      <td>1.927081</td>\n",
       "      <td>1.420443</td>\n",
       "      <td>1.450095</td>\n",
       "      <td>1.472359</td>\n",
       "      <td>1.613477</td>\n",
       "      <td>2.247674</td>\n",
       "      <td>2.446868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31058</th>\n",
       "      <td>HOUSEHOLD_1_004_CA_1_evaluation</td>\n",
       "      <td>1.342333</td>\n",
       "      <td>1.266069</td>\n",
       "      <td>1.035864</td>\n",
       "      <td>1.024562</td>\n",
       "      <td>1.553377</td>\n",
       "      <td>2.073160</td>\n",
       "      <td>1.839534</td>\n",
       "      <td>1.471478</td>\n",
       "      <td>1.334622</td>\n",
       "      <td>...</td>\n",
       "      <td>1.848246</td>\n",
       "      <td>1.741816</td>\n",
       "      <td>1.964079</td>\n",
       "      <td>1.310458</td>\n",
       "      <td>1.431128</td>\n",
       "      <td>1.185328</td>\n",
       "      <td>1.219093</td>\n",
       "      <td>1.659835</td>\n",
       "      <td>2.439339</td>\n",
       "      <td>2.160877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60977</th>\n",
       "      <td>FOODS_3_825_WI_3_evaluation</td>\n",
       "      <td>0.716163</td>\n",
       "      <td>0.626379</td>\n",
       "      <td>0.644473</td>\n",
       "      <td>0.647437</td>\n",
       "      <td>0.776059</td>\n",
       "      <td>0.854815</td>\n",
       "      <td>0.882837</td>\n",
       "      <td>0.754092</td>\n",
       "      <td>0.658399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869791</td>\n",
       "      <td>1.020404</td>\n",
       "      <td>1.035002</td>\n",
       "      <td>0.913090</td>\n",
       "      <td>0.836115</td>\n",
       "      <td>0.908147</td>\n",
       "      <td>0.707877</td>\n",
       "      <td>0.831467</td>\n",
       "      <td>0.881552</td>\n",
       "      <td>0.920081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    id        F1        F2        F3  \\\n",
       "30493    HOBBIES_1_004_CA_1_evaluation  1.904641  1.532269  1.585795   \n",
       "31058  HOUSEHOLD_1_004_CA_1_evaluation  1.342333  1.266069  1.035864   \n",
       "60977      FOODS_3_825_WI_3_evaluation  0.716163  0.626379  0.644473   \n",
       "\n",
       "             F4        F5        F6        F7        F8        F9  ...  \\\n",
       "30493  1.648829  1.926328  2.320598  2.625637  2.119428  1.699053  ...   \n",
       "31058  1.024562  1.553377  2.073160  1.839534  1.471478  1.334622  ...   \n",
       "60977  0.647437  0.776059  0.854815  0.882837  0.754092  0.658399  ...   \n",
       "\n",
       "            F19       F20       F21       F22       F23       F24       F25  \\\n",
       "30493  1.790031  2.283375  2.168417  1.927081  1.420443  1.450095  1.472359   \n",
       "31058  1.848246  1.741816  1.964079  1.310458  1.431128  1.185328  1.219093   \n",
       "60977  0.869791  1.020404  1.035002  0.913090  0.836115  0.908147  0.707877   \n",
       "\n",
       "            F26       F27       F28  \n",
       "30493  1.613477  2.247674  2.446868  \n",
       "31058  1.659835  2.439339  2.160877  \n",
       "60977  0.831467  0.881552  0.920081  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking pipeline results against final submission dataframe (the values are matching)\n",
    "sub = pd.read_csv('final_submission.csv')\n",
    "sub[sub['id'].isin(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0291c6",
   "metadata": {},
   "source": [
    "## 3.2 Performance Metric Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73538064",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sales.iloc[:,:-28]\n",
    "y = df_sales.iloc[:,-28:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "931dcfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<Preprocessing>>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10/10 [00:04<00:00,  2.23it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 8/8 [01:30<00:00, 11.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<Generating Forecasts>>>\n",
      "<<<Calculating Weights>>>\n",
      "<<<Calculating WRMSSE Score>>>\n",
      "<<<Model Evaluated>>>\n"
     ]
    }
   ],
   "source": [
    "wrmsse_val = performancePipe(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "34ee25e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Local WRMSSE on Validation: 0.5717035288102513\n"
     ]
    }
   ],
   "source": [
    "print('The Local WRMSSE on Validation:', wrmsse_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
